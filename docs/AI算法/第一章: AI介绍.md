# AI大模型

### 1， 什么是大模型
大模型（Large Language Models）是指拥有数十亿或数百亿个参数的大型预训练语言模型，如GPT系列模型。它们在解决各种自然语言处理任务方面表现出强大的能力，甚至可以展现出一些小规模语言模型所不具备的特殊能力，如涌现能力。LLM的研究已经成为当前AI界的热点，其技术发展将彻底改变我们开发和使用AI算法的时代。

### 2，语言建模
语言建模是提高机器语言智能的主要方法之一，一般来说，LM旨在对词序的生成概率进行建模，以预测未来tokens的概率
```js
统计语言模型 -> 神经语言模型 -> 预训练语言模型 -> 大模型
```

### 3, 统计语言模型
基于统计学习方法开发，例如根据最近的上下文预测下一个词。统计语言模型的一个经典例子是n-gram模型。n-gram模型中，一个词出现的概率只依赖于它前面的n-1个词。

### 4，神经语言模型
是使用神经网络来预测词序列的概率分布的模型。与传统的统计语言模型（n-gram模型）使用固定窗口大小的词来预测下一个词的概率不同，神经语言模型可以考虑更长的上下文或整个句子的信息。
循环神经网络（RNN）：包括LSTM和GRU等变体，能够处理变长的序列数据。
分布式表示：在神经语言模型中，每一个单词通常被编码为一个实数值向量，这些向量也被称为词嵌入（word embeddings）。词嵌入可以捕捉词与词之间的语义和语法关系

### 5，预训练语言模型（Pre-trained Language Model. PLM）
这些模型通常在大规模无标签预料库上进行预训练任务，学习词汇，短语，句子甚至跨句子的语言规律和知识。通过这种预训练，模型能够捕获广泛的通用语义特征，然后可以在特定任务上进行微调（fine-tuning），以适应特定的应用场景
Transformer模型通过其自注意力机制和高度的并行化能力，极大地提高来序列处理任务的效率和效果，它能够在处理序列数据师捕获全局依赖关系，同时具有并行计算的能力。是今年来自然语言处理领域的重要进展之一。

### 6，大语言模型（Large Language Models, LLM）
大语言模型就是大模型，指那些具有大量参数，在大规模数据集上训练的语言模型。这些模型能够理解和生成自然语言，通常是通过深度学习和自注意力机制（如Transformer架构）实现的。他们在自然语言处理（NLP）的多个领域都有广泛的应用，包括但不限于文本生成，翻译，摘要，问答和对话系统。
大语言模型通常有数十亿甚至数万亿个参数。例如：GPT-3拥有1750亿个参数<br/>
大模型应用：<br/>
ChatGPT<br/>
<strong>BERT vs GPT</strong><br/>
BERT(Bidirectional Encoder Representations from Transformers) 和GPT（Generative Pretrained Transformer）都是基于Transformers的架构。<br/>
(1) BERT<br/>
BERT是由Google AI 在2018年提出的一种预训练语言表示模型，它的主要特点是双向的Transformer编码器。这意味着BERT在处理一个单词时，会同时考虑这个单词前面和后面的上下文，这种全方位的上下文理解使得BERT在理解语言时更为精准。
(2) GPT<br/>
GPT有Open AI提出，是一种基于Transformer的预训练语言生成模型。与BERT不同，GPT使用的是单向的Transformer解码器。它在处理文本时主要关注当前单词之前的上下文，这使得GPT在生成连贯文本方面表现出色。

### 7， 大模型特点
* 参数数量庞大
* 数据需求巨大
* 计算资源密集
* 泛化能力强
* 迁移学习效果佳

### 8， 大模型问题
* 幻觉
* 资源消耗
* 数据偏见
* 可解释性差
* 安全性问题