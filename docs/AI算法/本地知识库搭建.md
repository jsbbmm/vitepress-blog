## 1, 为什么不直接使用网页版DeepSeek?
绝对的隐私保护，和个性化知识库构建
（1）数据隐私问题：互联网使用大模型数据隐私性无法得到绝对保证
（2）上传文件的限制，网页版AI对于文件上传的数量，大小一般有限制并且通常收费
（3） 上下文场景功能有限，开启新对话，需要重新上传附件

## 2，本地搭建网页版DeepSeek，可以解决那些问题
(1) 隐私保护
 * 本地部署对话大模型（如DeepSeek）
(2) 个性化知识库构建
 * 使用RAG技术，构建个人知识库，本地部署RAG技术所需要的开源框架RAGFlow
 * 本地部署Embedding大模型，（或着直接部署自带Embedding模型的RAGFlow版本）

## 3，为什么要使用RAG技术，RAG和模型微调的区别？
微调技术：在已有的训练模型基础上，在结合特定任务的数据集进一步对其进行训练，是的模型在这一领域中变现更好
RAG技术：在生成回答之前，通过信息检索从外部知识库中查找与问题相关的知识，增强生成过程中的信息来源，从而提升生成的质量和准确性。
RAG原理：
 * 检索（Retrieval）当用户提出问题时，系统从外部的知识库中检索与用户输入相关的内容。
 * 增强（Augmentation）系统将检索的信息与用户的输入结合，扩展模型的上下文，然后再传给生成的模型
 * 生成（Generation）生成模型基于增强后的输入生成最终的回答。由于这一回答参考啦外部知识库中的内容，因此更加准确可读

## 4，什么是Embedding，为什么还需要Embedding模型？
 * 准备外部知识库，可能是本地文件，搜索引擎结果，API等等
 * 通过Embedding模型，对知识库文件进行解析。Embedding的主要作用是将自然语言转化为机器可以理解的高维向量，并通过这一过程捕获到文本背后的语义信息
 * 通过Embedding模型，对用户的提问进行处理，用户的输入同样会经过嵌入（Embedding）处理，生成一个高维向量
 * 拿用户的提问去匹配本地知识库，使用用户输入生成的这个高维向量，去查询知识库中相关的文档片段。系统会利用某些相似度度量，去判断相似度
 
 模型分类： Chat模型， Embedding模型
 简而言之：Embedding模型是用来对你上传的附件进行解析的

 ## 5， 本地部署全流程
  * 1， 下载ollama, 通过ollama将DeepSeek模型下载到本地运行
  * 2， 下载RAGFlow源代码和Docker, 通过Docker本地部署RAGFlow,
  * 3， 在RAGFlow中构建个人知识库并实现基于个人知识库的对话问答
  
Docker镜像是一个封装好的环境，包含啦所有运行RAGFlow所需要的依赖，库和配置。
$ cd ragflow/docker
$ docker compose -f docker-compose.yml up -d

 ## 6, docker成功启动后，访问：
 http://localhost:80

  ## 7， 添加ollama模型
  ollama安装成功之后，要配置环境变量，让本地和loclahost可以访问
  ```js
  export OLLAMA_HOST=0.0.0.0:11434
  ```
  基础url: http://192.168.2.108:11434

  